{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bffb9b",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12446d2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "df = pd.read_csv('C:/Users/Rachele/PycharmProjects/topicmodeling/final_tweets.csv', encoding=\"utf-8\")\n",
    "list_of_tweets = df['tweets'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786a22a",
   "metadata": {},
   "source": [
    "## Removing non-English tweets from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e87d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "# import pre-trained model\n",
    "ft_model = ft.load_model(\"C:/Users/Rachele/Downloads/lid.176.bin\")\n",
    "\n",
    "english_tweets = []\n",
    "\n",
    "for tweet in list_of_tweets:\n",
    "    tweet = tweet.replace('\\n', \" \") # so that fasttext does not throw an error\n",
    "    prediction = ft_model.predict(tweet)\n",
    "    label = prediction[0]\n",
    "    if label == ('__label__en',):\n",
    "        english_tweets.append(tweet)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdeea62",
   "metadata": {},
   "source": [
    "## Data cleaning, tokenization, stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# function to remove graphical emojis\n",
    "def give_emoji_free_text(self, text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "# function to remove urls, mentions, hashtags and punctuation, lowercasing, tokenizing and stopwording\n",
    "def text_cleaning(text):\n",
    "    global clean_tweets\n",
    "    clean_tweets = []\n",
    "    for t in english_tweets:\n",
    "        clean_tweet = re.sub(r'(?:\\@|https?\\://)\\S+', '', t).lower()  # regex to remove urls and mentions, lowercasing\n",
    "        clean_tweet = re.sub('#[A-Za-z0-9_]+', '', clean_tweet)  # remove hashtags\n",
    "        clean_tweet = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', clean_tweet) # remove characters which are repeated more than twice\n",
    "        clean_tweet = emoji.replace_emoji(clean_tweet) # remove emojis\n",
    "        clean_tweet = clean_tweet.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "        clean_tweet = re.sub('([0-9]+)', '', clean_tweet) # remove numbers\n",
    "        clean_tweets.append(clean_tweet) # create list of cleaned tweets\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = []\n",
    "    for i in clean_tweets:\n",
    "        token = tokenizer.tokenize(i)\n",
    "        token = filter(lambda t: len(t) > 3, token) # exclude tokens shorter than 3 characters\n",
    "        tokens.append(token) # create list of tokens\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tweet = []\n",
    "    tweets = []\n",
    "    for tweet in tokens:\n",
    "        filtered_tweet = [w for w in tweet if not w in stop_words] # remove stopwords from list\n",
    "        tweets.append(filtered_tweet)\n",
    "    tweets = [x for x in tweets if x != []]\n",
    "    return tweets\n",
    "\n",
    "tweets = text_cleaning(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5097b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8adb9e",
   "metadata": {},
   "source": [
    "## Splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89de2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(tweets, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9601c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a9f9a",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de093f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# create dictionary\n",
    "corpus = [w for tweet in train for w in tweet] # flattening the nested list\n",
    "dictionary = corpora.Dictionary([corpus])\n",
    "\n",
    "# convert corpus into a bag of words\n",
    "bow = [dictionary.doc2bow(tweet) for tweet in train]\n",
    "\n",
    "# convert corpus into tf-idf and create the corpus\n",
    "tfidf = models.TfidfModel(bow, id2word=dictionary)\n",
    "corpus_tfidf = tfidf[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67509b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[300:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc879c",
   "metadata": {},
   "source": [
    "## Finding number of topics for highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coherence score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "for i in range(2,21):\n",
    "    lda_model = models.ldamodel.LdaModel(corpus_tfidf,\n",
    "                                         num_topics=i,\n",
    "                                         id2word=dictionary,\n",
    "                                         update_every=1,\n",
    "                                         chunksize=100,\n",
    "                                         passes=2,\n",
    "                                         alpha='auto')\n",
    "    coherence_model = CoherenceModel(model=lda_model,\n",
    "                                     texts=train,\n",
    "                                     dictionary=dictionary,\n",
    "                                     coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cef44",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training lda gensim model with tf-idf\n",
    "from gensim import models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(corpus_tfidf,\n",
    "                                     id2word=dictionary,\n",
    "                                     num_topics=14, # num of topics with highest coherence score\n",
    "                                     update_every=1,\n",
    "                                     chunksize=100,\n",
    "                                     passes=6,\n",
    "                                     alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d381b9",
   "metadata": {},
   "source": [
    "### Retrieving topics with top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903837f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 5 words with the strongest association to the derived topics\n",
    "for topic_num, words in lda_model.print_topics(num_words=15):\n",
    "    print('Words in {}: {}.'.format(topic_num, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca5393",
   "metadata": {},
   "source": [
    "### Computing coherence and perplexity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=train, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_lda = lda_model.log_perplexity(corpus_tfidf)\n",
    "print('\\nPerplexity: ', perplexity_lda)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b9c41",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa066b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#saving model to disk.\n",
    "\n",
    "temp_file = datapath(\"C:/Users/Rachele/Documents/GAlda_topics.model\")\n",
    "\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df1d41",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fca40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "#saving model to disk.\n",
    "\n",
    "temp_file = datapath(\"C:/Users/Rachele/Documents/GAlda_topics.model\")\n",
    "\n",
    "lda_model = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce864c77",
   "metadata": {},
   "source": [
    "## Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim_models.prepare(lda_model, corpus_tfidf, dictionary)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
